{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Tutorial in VSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "# pyspark type of data \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "# pyspark functions\n",
    "from pyspark.sql.functions import col, desc, round\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from delta import configure_spark_with_delta_pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use Spark with VSC please run this path if you need change path to SPARK_HOME where you install it\n",
    "os.environ['SPARK_HOME'] = \"/Users/maxrogowski/PycharmProjects/Spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'code'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "\n",
    "# path to downloaded spark-avro\n",
    "avro_jar_path = \"./spark-avro_2.12-3.5.1.jar\"\n",
    "\n",
    "# Add paths to PYSPARK_SUBMIT_ARGS to use both AVRO and Delta Lake\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--jars {avro_jar_path} pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 21:11:15 WARN Utils: Your hostname, Maxs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.57 instead (on interface en0)\n",
      "24/03/26 21:11:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/03/26 21:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Session\n",
    "spark = SparkSession.builder.appName(\"PySpark-Get-Started\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| Name|Body Age|\n",
      "+-----+--------+\n",
      "|  Max|      40|\n",
      "|  Aga|      14|\n",
      "|Tomas|      18|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the setup check if data is running on spark\n",
    "data = [(\"Max\", 40), (\"Aga\", 14), (\"Tomas\", 18)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Body Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD-Demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x127e0ef30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a SparkSession RDD - demo \n",
    "spark = SparkSession.builder.appName(\"RDD-Demo\").getOrCreate()\n",
    "# Perform operations using the SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put list to RDD\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)\n",
    "\n",
    "# Collect action: Retrieve all elements of the RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All elements of the rdd:  [('Max', 25), ('Tomas', 30), ('Aga', 35), ('Max', 40)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n",
      "The first element of the rdd:  ('Max', 25)\n",
      "The first two elements of the rdd:  [('Max', 25), ('Tomas', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Max\", 25), (\"Tomas\", 30), (\"Aga\", 35), (\"Max\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Print all elements\n",
    "print(\"All elements of the rdd: \", rdd.collect())\n",
    "\n",
    "# Count action: Count the number of elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)\n",
    "\n",
    "# First action: Retrieve the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)\n",
    "\n",
    "# Take action: Retrieve the n elements of the RDD\n",
    "taken_elements = rdd.take(2)\n",
    "print(\"The first two elements of the rdd: \", taken_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Tomas', 30)\n",
      "('Max', 25)\n",
      "('Aga', 35)\n",
      "('Max', 40)\n"
     ]
    }
   ],
   "source": [
    "# Pint in loop\n",
    "rdd.foreach(lambda x: print(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD with uppercase name:  [('MAX', 25), ('TOMAS', 30), ('AGA', 35), ('MAX', 40)]\n",
      "Filter records where age is greater than 30:  [('Aga', 35), ('Max', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "result = mapped_rdd.collect()\n",
    "print(\"RDD with uppercase name: \", result) \n",
    "\n",
    "# Filter transformation: Filter records where age is greater than 30\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "print(\"Filter records where age is greater than 30: \", filtered_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the total age for each name:  [('Max', 65), ('Tomas', 30), ('Aga', 35)]\n",
      "SortBy transformation: Sort the RDD by age in descending order:  [('Max', 40), ('Aga', 35), ('Tomas', 30), ('Max', 25)]\n"
     ]
    }
   ],
   "source": [
    "# ReduceByKey transformation: Calculate the total age for each name\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.collect()\n",
    "print(\"Calculate the total age for each name: \",  reduced_rdd.collect())\n",
    "\n",
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()\n",
    "print(\"SortBy transformation: Sort the RDD by age in descending order: \",  sorted_rdd.collect())\n",
    "\n",
    "# Save action: Save the RDD to a text file\n",
    "# Removing the existing output directory if it exists\n",
    "shutil.rmtree(\"data/output.txt\", ignore_errors=False)\n",
    "\n",
    "# Saving the RDD to a text file\n",
    "rdd.saveAsTextFile(\"data/output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create-DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Create-DataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrame from file\n",
    "df = spark.read.text(\"./data/data.txt\")\n",
    "\n",
    "# Group by words and count to create statistic\n",
    "result_df = df.selectExpr(\"explode(split(value, ' ')) as word\").groupBy(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='the', count=12),\n",
       " Row(word='of', count=7),\n",
       " Row(word='a', count=7),\n",
       " Row(word='in', count=5),\n",
       " Row(word='distributed', count=5),\n",
       " Row(word='Spark', count=4),\n",
       " Row(word='API', count=3),\n",
       " Row(word='RDD', count=3),\n",
       " Row(word='is', count=3),\n",
       " Row(word='on', count=3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first 10 most popular word \n",
    "result_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV file into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open file using bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,category,quantity,price\n",
      "1,iPhone 12,Electronics,10,899.99\n",
      "2,Nike Air Max 90,Clothing,25,119.99\n",
      "3,KitchenAid Stand Mixer,Home Appliances,5,299.99\n",
      "4,The Great Gatsby,Books,50,12.99\n",
      "5,L'Oreal Paris Mascara,Beauty,100,9.99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 6 ./data/products.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV with header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with header\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema price is not correct it is a string to read it correctly we can use schema like in db\n",
    "\n",
    "# Define the schema \n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "    # type quantity is Double not string\n",
    "    StructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "    # type price is Double not string\n",
    "    StructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with correct schema definition\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "|  6|            Yoga Mat|         Sports|      30| 29.99|\n",
      "|  7| Samsung 4K Smart TV|    Electronics|       8|799.99|\n",
      "|  8|        Levi's Jeans|       Clothing|      15| 49.99|\n",
      "|  9|Dyson Vacuum Cleaner|Home Appliances|       3|399.99|\n",
      "| 10| Harry Potter Series|          Books|      20| 15.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Print content of DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV with inferSchema - spark defines the data types itself \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into DataFrame with inferSchema\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Print content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON file into DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single line json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":1,\"name\":\"iPhone 12\",\"category\":\"Electronics\",\"quantity\":10,\"price\":899.99}\n",
      "{\"id\":2,\"name\":\"Nike Air Max 90\",\"category\":\"Clothing\",\"quantity\":25,\"price\":119.99}\n",
      "{\"id\":3,\"name\":\"KitchenAid Stand Mixer\",\"category\":\"Home Appliances\",\"quantity\":5,\"price\":299.99}\n",
      "{\"id\":4,\"name\":\"The Great Gatsby\",\"category\":\"Books\",\"quantity\":50,\"price\":12.99}\n",
      "{\"id\":5,\"name\":\"L'Oreal Paris Mascara\",\"category\":\"Beauty\",\"quantity\":100,\"price\":9.99}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 5 ./data/products_singleline.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read single line JSON\n",
    "# Each row is a JSON record, records are separated by new line\n",
    "json_file_path = \"./data/products_singleline.json\"\n",
    "df = spark.read.json(json_file_path)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Print content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat ./data/products_singleline.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"name\": \"iPhone 12\",\n",
      "    \"category\": \"Electronics\",\n",
      "    \"quantity\": 10,\n",
      "    \"price\": 899.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"name\": \"Nike Air Max 90\",\n",
      "    \"category\": \"Clothing\",\n",
      "    \"quantity\": 25,\n",
      "    \"price\": 119.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"name\": \"KitchenAid Stand Mixer\",\n",
      "    \"category\": \"Home Appliances\",\n",
      "    \"quantity\": 5,\n",
      "    \"price\": 299.99\n",
      "  },\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n 22 ./data/products_multiline.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multi-line JSON\n",
    "# JSON is an array of record, records are separated by a comma each record is defined in multiple lines\n",
    "json_file_path = \"./data/products_multiline.json\"\n",
    "df = spark.read.json(json_file_path, multiLine=True)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Print content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and Read parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame into parquet file\n",
    "parquet_file_path = \"./data/products.parquet\"\n",
    "df.write.mode('overwrite').parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from Parquet file\n",
    "df = spark.read.parquet(parquet_file_path)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Print content of DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations: Select, Filter, GroupBy, Join, Sort, Distinct, Drop, WithColumn, Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,category,quantity,price\n",
      "1,iPhone,Electronics,10,899.99\n",
      "2,Macbook,Electronics,5,1299.99\n",
      "3,iPad,Electronics,15,499.99\n",
      "4,Samsung TV,Electronics,8,799.99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -5 data/stocks.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "Initial DataFrame:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrame-Operations\").getOrCreate()\n",
    "\n",
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/stocks.txt\"\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "selected_columns = df.select(\"id\", \"name\", \"price\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data: 12\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "|  6|    Nike Shoes|   Clothing|      30|99.99|\n",
      "|  7|  Adidas Shoes|   Clothing|      25|89.99|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 28|         Jeans|   Clothing|      30|59.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 30|      Sneakers|   Clothing|      40|79.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "\n",
      "Grouped and Aggregated Data:\n",
      "+-----------+-------------+------------------+\n",
      "|   category|sum(quantity)|        avg(price)|\n",
      "+-----------+-------------+------------------+\n",
      "|       Food|          450|2.2960000000000003|\n",
      "|     Sports|           35|             34.99|\n",
      "|Electronics|           98| 586.6566666666665|\n",
      "|   Clothing|          200|  99.2757142857143|\n",
      "|  Furniture|           41|            141.99|\n",
      "|Accessories|           55|             27.49|\n",
      "+-----------+-------------+------------------+\n",
      "\n",
      "Joined Data:\n",
      "+---+----------------+-----------+--------+-------+------------------+\n",
      "| id|            name|   category|quantity|  price|secondary category|\n",
      "+---+----------------+-----------+--------+-------+------------------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|       Electronics|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|       Electronics|\n",
      "|  3|            iPad|Electronics|      15| 499.99|       Electronics|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|       Electronics|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|       Electronics|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|          Clothing|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|          Clothing|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|       Electronics|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|       Electronics|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|         Furniture|\n",
      "+---+----------------+-----------+--------+-------+------------------+\n",
      "\n",
      "Data Sorted DESC:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "| 26|          Camera|Electronics|      10| 599.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Data Sorted ASC:\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 19|      Yoga Mat|     Sports|      20|19.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 20|  Dumbbell Set|     Sports|      15|49.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Sorted Data Descending:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "| 26|          Camera|Electronics|      10| 599.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 18|     Winter Coat|   Clothing|      10| 149.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = df.filter(df.quantity > 20)\n",
    "print(\"Filtered Data:\", filtered_data.count())\n",
    "filtered_data.show()\n",
    "\n",
    "# GroupBy and Aggregations Group By: Group data based on category, data is sum quantity and average price\n",
    "grouped_data = df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "print(\"Grouped and Aggregated Data:\")\n",
    "grouped_data.show()\n",
    "\n",
    "# Join with another DataFrame to avoid duplicate category rename it to secondary category\n",
    "df2 = df.select(\"id\", \"category\").limit(10).withColumnRenamed(\"category\", \"secondary category\")\n",
    "joined_data = df.join(df2, \"id\", \"inner\")\n",
    "print(\"Joined Data:\")\n",
    "joined_data.show()\n",
    "\n",
    "# Sort by a column\n",
    "sorted_data = df.orderBy(desc(\"price\"))\n",
    "print(\"Data Sorted DESC:\")\n",
    "sorted_data.show(10)\n",
    "\n",
    "# Sort by a column\n",
    "sorted_data = df.orderBy(\"price\")\n",
    "print(\"Data Sorted ASC:\")\n",
    "sorted_data.show(10)\n",
    "\n",
    "# Sort by a column desc by 2 columns\n",
    "sorted_data = df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "print(\"Sorted Data Descending:\")\n",
    "sorted_data.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Product Categories:\n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|       Food|\n",
      "|     Sports|\n",
      "|Electronics|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "|Accessories|\n",
      "+-----------+\n",
      "\n",
      "Dropped Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame with New 'revenue' Column:\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "| id|            name|   category|quantity|  price|revenue|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99| 8999.9|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|6499.95|\n",
      "|  3|            iPad|Electronics|      15| 499.99|7499.85|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|6399.92|\n",
      "|  5|           LG TV|Electronics|      10| 699.99| 6999.9|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99| 2999.7|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|2249.75|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|1799.88|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99| 3999.8|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99| 2499.9|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame with Aliased Column:\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "| id|            name|   category|quantity|product_price|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "|  1|          iPhone|Electronics|      10|       899.99|\n",
      "|  2|         Macbook|Electronics|       5|      1299.99|\n",
      "|  3|            iPad|Electronics|      15|       499.99|\n",
      "|  4|      Samsung TV|Electronics|       8|       799.99|\n",
      "|  5|           LG TV|Electronics|      10|       699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|        99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|        89.99|\n",
      "|  8| Sony Headphones|Electronics|      12|       149.99|\n",
      "|  9|Beats Headphones|Electronics|      20|       199.99|\n",
      "| 10|    Dining Table|  Furniture|      10|       249.99|\n",
      "+---+----------------+-----------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get distinct product category (Get unique rows)\n",
    "distinct_rows = df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()\n",
    "\n",
    "# Drop: Remove specified columns.\n",
    "dropped_columns = df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns:\")\n",
    "dropped_columns.show(10)\n",
    "\n",
    "# WithColumn: Add new calculated columns.\n",
    "df_with_new_column = df.withColumn(\"revenue\", df.quantity * df.price)\n",
    "print(\"DataFrame with New 'revenue' Column:\")\n",
    "df_with_new_column.show(10)\n",
    "\n",
    "\n",
    "# Alias: Rename columns for better readability.\n",
    "df_with_alias = df.withColumnRenamed(\"price\", \"product_price\")\n",
    "print(\"DataFrame with Aliased Column:\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "Initial DataFrame:\n",
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|        Jane Smith| 25|Female| 45000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "+------------------+---+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameSQL\").getOrCreate()\n",
    "\n",
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/persons.csv\"\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table (dump DataFrame to table)\n",
    "df.createOrReplaceTempView(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where age is greater than 25\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 25\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|avg_salary|\n",
      "+------+----------+\n",
      "|Female|   52300.0|\n",
      "|  Male|   62100.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average salary by gender\n",
    "avg_salary_by_gender = spark.sql(\"SELECT gender, AVG(salary) as avg_salary FROM my_table GROUP BY gender\")\n",
    "avg_salary_by_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+\n",
      "|gender|age_group|avg_salary|\n",
      "+------+---------+----------+\n",
      "|  Male|       40|   75000.0|\n",
      "|Female|       30|   59000.0|\n",
      "|  Male|       30|  63142.86|\n",
      "|Female|       20|  47833.33|\n",
      "|  Male|       20|   52000.0|\n",
      "+------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average salary by gender and age, rounded to every 10 years\n",
    "avg_salary_by_gender_age = spark.sql(\"SELECT gender, FLOOR(age/10)*10 AS age_group, AVG(salary) AS avg_salary FROM my_table GROUP BY gender, age_group ORDER BY age_group DESC, gender ASC\")\n",
    "avg_salary_by_gender_age = avg_salary_by_gender_age.withColumn(\"avg_salary\", round(avg_salary_by_gender_age[\"avg_salary\"], 2))\n",
    "avg_salary_by_gender_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and managing temporary views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Query the temporary view\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 25\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If view exist nr.1: True\n",
      "If view exist nr.2: False\n"
     ]
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists_1 = spark.catalog.tableExists(\"people\")\n",
    "print(f'If view exist nr.1: {view_exists_1}')\n",
    "\n",
    "# Drop a temporary view\n",
    "spark.catalog.dropTempView(\"people\")\n",
    "\n",
    "# Check if a temporary view exists\n",
    "view_exists_2 = spark.catalog.tableExists(\"people\")\n",
    "\n",
    "print(f'If view exist nr.2: {view_exists_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subquries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|   John|\n",
      "|  2|  Alice|\n",
      "|  3|    Bob|\n",
      "|  4|  Emily|\n",
      "|  5|  David|\n",
      "|  6|  Sarah|\n",
      "|  7|Michael|\n",
      "|  8|   Lisa|\n",
      "|  9|William|\n",
      "+---+-------+\n",
      "\n",
      "+----------+---+------+\n",
      "|department| id|salary|\n",
      "+----------+---+------+\n",
      "|        HR|  1| 66000|\n",
      "|        HR|  2| 55000|\n",
      "|        HR|  3| 58000|\n",
      "|        IT|  4| 70000|\n",
      "|        IT|  5| 72000|\n",
      "|        IT|  6| 68000|\n",
      "|     Sales|  7| 75000|\n",
      "|     Sales|  8| 78000|\n",
      "|     Sales|  9| 77000|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"),\n",
    "    (9, \"William\")\n",
    "]\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 66000), (\"HR\", 2, 55000), (\"HR\", 3, 58000),\n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000),\n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)\n",
    "]\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "employees.show()\n",
    "\n",
    "salaries.show()\n",
    "\n",
    "# Register as temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(salary)|\n",
      "+-----------------+\n",
      "|68777.77777777778|\n",
      "+-----------------+\n",
      "\n",
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|  Emily| 70000|\n",
      "|  David| 72000|\n",
      "|Michael| 75000|\n",
      "|   Lisa| 78000|\n",
      "|William| 77000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get average salary\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT AVG(salary) FROM salaries\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# Subquery to find employees with salaries above average and add salary \n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT employees.name, salaries.salary\n",
    "    FROM employees\n",
    "    JOIN salaries ON employees.id = salaries.id\n",
    "    WHERE salaries.salary > (\n",
    "        SELECT AVG(salary)\n",
    "        FROM salaries\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+\n",
      "|department| id|salary|   name|\n",
      "+----------+---+------+-------+\n",
      "|        HR|  1| 66000|   John|\n",
      "|        HR|  2| 55000|  Alice|\n",
      "|        HR|  3| 58000|    Bob|\n",
      "|        IT|  4| 70000|  Emily|\n",
      "|        IT|  5| 72000|  David|\n",
      "|        IT|  6| 68000|  Sarah|\n",
      "|     Sales|  7| 75000|Michael|\n",
      "|     Sales|  9| 77000|William|\n",
      "|     Sales|  8| 78000|   Lisa|\n",
      "+----------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = spark.sql(\"\"\"\n",
    "    select  salaries.*, employees.name\n",
    "    from salaries \n",
    "    left join employees on salaries.id = employees.id\n",
    "\"\"\")\n",
    "\n",
    "employee_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+----+\n",
      "|department| id|salary|   name|rank|\n",
      "+----------+---+------+-------+----+\n",
      "|        HR|  1| 66000|   John|   1|\n",
      "|        HR|  3| 58000|    Bob|   2|\n",
      "|        HR|  2| 55000|  Alice|   3|\n",
      "|        IT|  5| 72000|  David|   1|\n",
      "|        IT|  4| 70000|  Emily|   2|\n",
      "|        IT|  6| 68000|  Sarah|   3|\n",
      "|     Sales|  8| 78000|   Lisa|   1|\n",
      "|     Sales|  9| 77000|William|   2|\n",
      "|     Sales|  7| 75000|Michael|   3|\n",
      "+----------+---+------+-------+----+\n",
      "\n",
      "+----------+---+------+-----+----+\n",
      "|department| id|salary| name|rank|\n",
      "+----------+---+------+-----+----+\n",
      "|        HR|  1| 66000| John|   1|\n",
      "|        IT|  5| 72000|David|   1|\n",
      "|     Sales|  8| 78000| Lisa|   1|\n",
      "+----------+---+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "\n",
    "# Calculate the rank of employees within each department based on salary\n",
    "employee_salary_with_rank = employee_salary.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "employee_salary_with_rank.show()\n",
    "\n",
    "# Get name with best salary per department\n",
    "employee_salary_with_rank.filter(employee_salary_with_rank.rank == 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Preview:\n",
      "                  name  age  gender  salary\n",
      "0             John Doe   30    Male   50000\n",
      "1           Jane Smith   25  Female   45000\n",
      "2        David Johnson   35    Male   60000\n",
      "3          Emily Davis   28  Female   52000\n",
      "4       Michael Wilson   40    Male   75000\n",
      "5          Sarah Brown   32  Female   58000\n",
      "6           Robert Lee   29    Male   51000\n",
      "7          Lisa Garcia   27  Female   49000\n",
      "8       James Martinez   38    Male   70000\n",
      "9   Jennifer Rodriguez   26  Female   47000\n",
      "10    William Anderson   33    Male   62000\n",
      "11     Karen Hernandez   31  Female   55000\n",
      "12  Christopher Taylor   37    Male   69000\n",
      "13       Mary Gonzalez   24  Female   44000\n",
      "14       Matthew Davis   36    Male   67000\n",
      "15      Patricia White   29  Female   50000\n",
      "16       Daniel Miller   34    Male   64000\n",
      "17   Elizabeth Jackson   30  Female   52000\n",
      "18       Joseph Harris   28    Male   53000\n",
      "19        Linda Martin   39  Female   71000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>30</td>\n",
       "      <td>Male</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>45000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Johnson</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emily Davis</td>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>52000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michael Wilson</td>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sarah Brown</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Robert Lee</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>51000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lisa Garcia</td>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>49000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>James Martinez</td>\n",
       "      <td>38</td>\n",
       "      <td>Male</td>\n",
       "      <td>70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jennifer Rodriguez</td>\n",
       "      <td>26</td>\n",
       "      <td>Female</td>\n",
       "      <td>47000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>William Anderson</td>\n",
       "      <td>33</td>\n",
       "      <td>Male</td>\n",
       "      <td>62000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Karen Hernandez</td>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Christopher Taylor</td>\n",
       "      <td>37</td>\n",
       "      <td>Male</td>\n",
       "      <td>69000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mary Gonzalez</td>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>44000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Matthew Davis</td>\n",
       "      <td>36</td>\n",
       "      <td>Male</td>\n",
       "      <td>67000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Patricia White</td>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Daniel Miller</td>\n",
       "      <td>34</td>\n",
       "      <td>Male</td>\n",
       "      <td>64000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Elizabeth Jackson</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>52000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Joseph Harris</td>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>53000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Linda Martin</td>\n",
       "      <td>39</td>\n",
       "      <td>Female</td>\n",
       "      <td>71000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  age  gender  salary\n",
       "0             John Doe   30    Male   50000\n",
       "1           Jane Smith   25  Female   45000\n",
       "2        David Johnson   35    Male   60000\n",
       "3          Emily Davis   28  Female   52000\n",
       "4       Michael Wilson   40    Male   75000\n",
       "5          Sarah Brown   32  Female   58000\n",
       "6           Robert Lee   29    Male   51000\n",
       "7          Lisa Garcia   27  Female   49000\n",
       "8       James Martinez   38    Male   70000\n",
       "9   Jennifer Rodriguez   26  Female   47000\n",
       "10    William Anderson   33    Male   62000\n",
       "11     Karen Hernandez   31  Female   55000\n",
       "12  Christopher Taylor   37    Male   69000\n",
       "13       Mary Gonzalez   24  Female   44000\n",
       "14       Matthew Davis   36    Male   67000\n",
       "15      Patricia White   29  Female   50000\n",
       "16       Daniel Miller   34    Male   64000\n",
       "17   Elizabeth Jackson   30  Female   52000\n",
       "18       Joseph Harris   28    Male   53000\n",
       "19        Linda Martin   39  Female   71000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definition of a function that formats the table appearance using pandas\n",
    "def pretty_print(df):\n",
    "    print(\"Table Preview:\")\n",
    "    df_pandas = df.toPandas()\n",
    "    print(df_pandas)\n",
    "    \n",
    "pretty_print(df)\n",
    "\n",
    "# Print just using pandas \n",
    "df_pandas = df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro file format Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "Initial DataFrame:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Display loaded data:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "| 11|      Study Desk|  Furniture|       8| 149.99|\n",
      "| 12|          Apples|       Food|     100|    0.5|\n",
      "| 13|         Bananas|       Food|     150|   0.25|\n",
      "| 14|         Oranges|       Food|     120|   0.75|\n",
      "| 15|  Chicken Breast|       Food|      50|   3.99|\n",
      "| 16|   Salmon Fillet|       Food|      30|   5.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "| 18|     Winter Coat|   Clothing|      10| 149.99|\n",
      "| 19|        Yoga Mat|     Sports|      20|  19.99|\n",
      "| 20|    Dumbbell Set|     Sports|      15|  49.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"AvroExample\").config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.2.0\").getOrCreate()\n",
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/stocks.txt\"\n",
    "df = spark.read.csv(data_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema of DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(10)\n",
    "\n",
    "# Convert DataFrame to Avro format\n",
    "df.write.format(\"avro\").mode(\"overwrite\").save(\"data/operations.avro\")\n",
    "\n",
    "# Load data in Avro format\n",
    "df_avro = spark.read.format(\"avro\").load(\"data/operations.avro\")\n",
    "\n",
    "# Display loaded data\n",
    "print(\"Display loaded data:\")\n",
    "df_avro.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
